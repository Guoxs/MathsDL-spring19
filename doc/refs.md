<a name="lec1"></a>
## Lecture 1: The Curse of Dimensionality

### Main References 
* ["Breaking the curse of dimensionality with convex neural networks", F. Bach](http://www.jmlr.org/papers/volume18/14-546/14-546.pdf)

* ["Understanding Machine Learning: From Theory to Algorithms", S. Shalev-Swartz, Ben-David](https://www.amazon.com/Understanding-Machine-Learning-Theory-Algorithms/dp/1107057132)

* ["Nesterov Punctuated Equilibrium", argmin post by Frostig & Recht](http://www.argmin.net/2017/04/03/evolution/)

* ["Failures of Gradient-Based Deep Learning", S. Shalev-Shwartz et al.](https://arxiv.org/pdf/1703.07950.pdf)

### Further References

* ["Introduction to Non-parametric Estimation", A. Tsybakov](http://www.springer.com/us/book/9780387790510)

* ["EQUIVALENCE OF DISTANCE-BASED AND RKHS-BASED STATISTICS IN HYPOTHESIS TESTING", Sejdinovic et al](https://arxiv.org/pdf/1207.6076.pdf)

* ["A primer on OT", M.Cuturi and J.Solomon, NIPS'17 tutorial](https://www.dropbox.com/s/55tb2cf3zipl6xu/aprimeronOT.pdf?dl=0)

* ["Sharp asymptotic and finite-sample rates of convergence of empirical measures in Wasserstein distance", J.Weed and F.Bach](https://arxiv.org/pdf/1707.00087.pdf)

* ["High-Dimensional Probability", R. Vershynin](https://www.math.uci.edu/~rvershyn/papers/HDP-book/HDP-book.pdf)

* ["Random Gradient-Free Minimization of Convex Functions", Y.Nesterov](https://pdfs.semanticscholar.org/8427/2faaaf0074b461570e5bb48514ac2c94aa72.pdf)



<a name="lec2"></a>
## Lecture 2: Geometric Stability in Euclidean Domains. 

### Main References: 

* [Group Invariant Scattering](https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf), S. Mallat

* [Invariant Scattering Convolutional Networks](http://ieeexplore.ieee.org/abstract/document/6522407/), J.Bruna, S. Mallat


<a name="lec3"></a>
## Lecture 3: The Scattering Transform and Beyond

### Main References: 

* [Group Invariant Scattering](https://www.di.ens.fr/~mallat/papiers/ScatCPAM.pdf), S. Mallat

* [Scattering Representations for Recognition](https://pastel.archives-ouvertes.fr/file/index/docid/905109/filename/phdmain_final.pdf), J.Bruna PhD Thesis.

* [Rotation, Scaling and Deformation Invariant Scattering for Texture Discrimination](https://www.cv-foundation.org/openaccess/content_cvpr_2013/papers/Sifre_Rotation_Scaling_and_2013_CVPR_paper.pdf), Sifre and Mallat, CVPR15.

### Further References:

* [Exponential Decay of Scattering Coefficients](http://ieeexplore.ieee.org/abstract/document/8024473/), I. Waldspurger.

* [Analysis of Time-Frequency Scattering Transforms](https://www.sciencedirect.com/science/article/pii/S1063520317300933), Czaja and Li.

* [Energy Propagation in Deep Convolutional Neural Networks](http://ieeexplore.ieee.org/abstract/document/8051085/), Wiatowski et al. 

<a name="lec4"></a>
## Lecture 4: Non-Euclidean Geometric Stability and Graph Neural Networks

### Main References:

* [Geometric Deep Learning: Going beyond Euclidean Data](http://geometricdeeplearning.com), M. Bronstein et al, 17

### Further References:

* [i-RevNet: Deep Invertible Networks](https://openreview.net/forum?id=HJsjkMb0Z), Jacobsen, Smeulders, Oyallon, ICLR'18

* [Spherical CNNs](https://openreview.net/forum?id=Hkbd5xZRb), Cohen, Welling et al, ICLR'18

* [Deep Image Prior](https://arxiv.org/pdf/1711.10925v2.pdf), Ulyanov, Vedaldi et al,'17

* [Community Detection with Graph Neural Networks](https://arxiv.org/pdf/1705.08415.pdf), B. and Li'18


<a name="lec5"></a>
## Lecture 5: Graph Neural Network Applications

### Main References:

* [Geometric Deep Learning: Going beyond Euclidean Data](http://geometricdeeplearning.com), M. Bronstein et al, 17

* [Community Detection with Graph Neural Networks](https://arxiv.org/pdf/1705.08415.pdf), B. and Li'18

* [Neural Message Passing for Quantum Chemistry, Gilmer et al.17](https://arxiv.org/abs/1704.01212)

### Further References:

* [Semi-Supervised Classification with Graph Convolutional Networks, Kipf & Welling](https://arxiv.org/abs/1609.02907)

* [Representation Learning on Graphs: Methods and Applications, Hamilton, Ying and Leskovec](https://arxiv.org/abs/1709.05584)

* [Quadratic Assignment with Graph Neural Networks, Nowak et al](https://arxiv.org/abs/1706.07450)

<a name="lec6"></a>
## Lecture 6: Unsupervised Learning under Geometric Priors

### Main References:

* [Geometrical Insights for Implicit Generative Modelling, Bottou et al. '18](https://arxiv.org/pdf/1712.07822.pdf)

* [Sparse Multiscale Microcanonical Models, B. and Mallat'18](https://arxiv.org/abs/1801.02013)

### Further References:

* [Autoencoding Variational Bayes, Kingma & Welling'14](https://arxiv.org/abs/1312.6114).

* [Generalization and Equilibrium in GANs, Arora et al](https://arxiv.org/pdf/1703.00573.pdf)


<a name="lec7"></a>
## Lecture 7: Discrete vs Continuous Time Optimization: The Convex Case

### Main References:

* [Large-scale Machine Learning and convex optimization, F. Bach, 17](http://www.di.ens.fr/%7Efbach/fbach_frejus_2017.pdf)

* [A differential Equation for Modelling Nesterov's Accelerated Gradient Method, Su, Boyd, Candes, '14](https://arxiv.org/abs/1503.01243)



### Further References

* [Convex Optimization, Bubeck'15](https://arxiv.org/abs/1405.4980)

* [A Lyapunov Analysis of Momentum Methods in Optimization, Wilson, Recht, Jordan'18](https://arxiv.org/abs/1611.02635)

* [On Symplectic Optimization, Betancourt, Jordan, Wilson](https://arxiv.org/abs/1802.03653)

<a name="lec8"></a>
## Lecture 8: Discrete vs Continuous Time Optimization: Stochastic and Nonconvex case

### Main References:

* [A Variational Perspective on Accelerated Methods in Optimization, Wibisono, Wilson and Jordan](http://www.pnas.org/content/113/47/E7351?etoc=)

* [Bridging the Gap between Constant Step size Stochastic Gradient Descent and Markov Chains, Dieuleveut, Durmus, Bach](https://arxiv.org/abs/1707.06386)



<a name="lec9"></a>
## Lecture 9: Discrete vs Continuous Time Optimization: Stochastic and Nonconvex case

### Main References:

* [Stochastic Modified Equations and Adaptive Stochastic Gradient Descent Algorithms, Li, Tai & E](https://arxiv.org/abs/1511.06251)

* [Non-Convex Learning via Stochastic Gradient Langevin Dynamics: A nonasymptotic Analysis, Raginsky, Rakhlin, Telgarsky](https://arxiv.org/abs/1702.03849)

* [Fokker-Plank Equations, Pavlotis Lecture Notes](http://wwwf.imperial.ac.uk/~pavl/lec_fokker_planck.pdf)

<a name="lec10"></a>
## Lecture 10: Nonconvex Optimization

### Main References:

* [Gradient Descent Converges to Minimisers, Lee et al.'16](https://arxiv.org/pdf/1710.07406.pdf)

* [Gradient Descent can take exponential time to escape saddle points, Lee et al.'17](http://papers.nips.cc/paper/6707-gradient-descent-can-take-exponential-time-to-escape-saddle-points.pdf)

* [Escaping from Saddle points-- online stochastic gradient for tensor decomposition, Ge et al.'15](https://arxiv.org/abs/1503.02101)

* [Deep Learning without poor local minima, Kawaguchi'16](http://papers.nips.cc/paper/6111-deep-learning-without-poor-local-minima)

### Further References

* [How to escape Saddle points efficiently, Jin et al.'17](https://arxiv.org/abs/1703.00887)


<a name="lec11"></a>
## Lecture 11: Landscape of Optimization

### Main References:

* [Random Matrices and the complexity of Spin Glasses, Auffinger et al'10](https://arxiv.org/abs/1003.1129)

* [Complex energy landscapes in spiked-tensor and simple glassy models: ruggedness, arrangements of local minima and phase transitions, Ross, Ben Arous, Biroli, Camarotta](https://arxiv.org/pdf/1804.02686.pdf)

* [Neural Networks with Finite Intrinsic Dimension have no Spurious Valleys, Venturi et al.](https://arxiv.org/abs/1802.06384)

* [Topology and Geometry of Half-Rectified Network Optimization, Freeman et al](https://arxiv.org/abs/1611.01540)

### Further References

* [On the Optimization Lanscape of Tensor Decomposition, Ge and Ma](https://arxiv.org/pdf/1706.05598.pdf)

* [The landscape of the spiked tensor model, Ben Arous et al](https://arxiv.org/pdf/1711.05424.pdf)


<a name="lec12"></a>
## Lecture 12: Guest Lecture Behnam Neyshabur (IAS/NYU): Generalization in Deep Learning

### Main References:

* [Understanding Machine Learning: From Theory to Algorithms. Shai Shalev-Shwartz and Shai Ben-David. Cambridge University Press, 2014](https://www.cs.huji.ac.il/~shais/UnderstandingMachineLearning/understanding-machine-learning-theory-algorithms.pdf): Part I (Foundations) and Part IV (Advanced Theory).

### Further References:

* [Implicit Regularization in Deep Learning. Behnam Neyshabur. PhD Thesis, 2017](https://arxiv.org/pdf/1709.01953.pdf). Part I (Implicit Regularization and Generalization)

* [Spectrally-normalized margin bounds for neural networks. Bartlett, Peter L., Dylan J. Foster, and Matus J. Telgarsky. NIPS 2017](https://arxiv.org/pdf/1706.08498.pdf)

* [Computing Nonvacuous Generalization Bounds for Deep (Stochastic) Neural Networks with Many More Parameters than Training Data. Dziugaite, Gintare Karolina, and Daniel M. Roy. UAI 2017](https://arxiv.org/pdf/1703.11008v2.pdf)

